# -*- coding: utf-8 -*-
"""forgeall_ssae_model_cross_validator.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qp3eMsHasgARohwF0Ok9GV2_5McXP7Xz
"""

from sklearn.metrics import make_scorer, mean_squared_error, r2_score
import numpy as np
# Perform cross-validation on the test set using the best model
from sklearn.model_selection import cross_validate


# Define a function to calculate RMSE and MAE as scorers
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# Define scoring metrics for cross-validation
scoring = {
    'MSE': make_scorer(mean_squared_error),
    'RMSE': make_scorer(rmse),
    'MAE': make_scorer(mae),
    'R2': make_scorer(r2_score)
}

def fa_perform_ssae_model_cross_validations(best_knn, latent_train_scaled, train_labels):
    '''
    #first with original data
    cv_results = cross_validate(
        best_knn_orig,
        train_data_scaled,
        train_labels,
        cv=10,
        scoring=scoring,
        n_jobs=-1
    )

    # Display cross-validation performance metrics
    print("\nCross-Validation Performance on Original Test Data:")
    for metric, scores in cv_results.items():
        if 'test' in metric:  # Filter only test scores
            metric_name = metric.split('_')[-1]
            print(f"{metric_name}: Mean={scores.mean():.4f}, Std={scores.std():.4f}")
    '''
    #second on latent space data
    cv_results = cross_validate(
        best_knn,
        latent_train_scaled,
        train_labels,
        cv=10,
        scoring=scoring,
        n_jobs=-1
    )

    # Display cross-validation performance metrics
    print("\nCross-Validation Performance on Latent Test Data:")
    for metric, scores in cv_results.items():
        if 'test' in metric:  # Filter only test scores
            metric_name = metric.split('_')[-1]
            print(f"{metric_name}: Mean={scores.mean():.4f}, Std={scores.std():.4f}")