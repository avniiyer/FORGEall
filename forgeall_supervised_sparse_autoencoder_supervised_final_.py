# -*- coding: utf-8 -*-
"""FORGEall Supervised Sparse Autoencoder Supervised - FINAL .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Fa1OCtjm6UsvY80NXHlP4JJ-uVIJsnw
"""

'''
1. This v8 code below is SUPERVISED version of the code for HEA model performance.
2. The data file used as input is: df_298_cleaned_filtered_1 (w_entropy).csv
3. The code also depends on a utility .py file called general_file_utilities.py for some file and stats related functionality
4. The supervised model has several hyper params:
        These two are passed in as part of model compile
            loss_weight_sparse
            loss_weight_divergence
        The others are part of model creation, sent in as a param grid dictionary. Param grid will run permutations to find best model and model params.
          param_grid = {
            'hidden_dim': [8, 10, 12],
            'latent_dim': [4, 6, 8],
            'target_sparsity': [0.05, 0.08 ],
            'batch_size': [8]
          }

'''

!pip install GPUtil

"""# << - - - SETUP Profiling - - ->>"""

from tensorflow.keras.callbacks import TensorBoard

# Define a log directory for storing logs
log_dir = "./logs"

# Create a TensorBoard callback with profiling enabled
tensorboard_callback = TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    profile_batch='5,15'  # Profile batches 5 to 15
)
log_dir = "./logs"

# Create a TensorBoard callback with profiling enabled
tensorboard_callback = TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    profile_batch='5,15'  # Profile batches 5 to 15
)

!pip install pytz

def print_current_timestamp_in_pst():
    """
    Prints the current timestamp in Pacific Standard Time (PST).
    """


    from datetime import datetime
    import pytz

    # Get the current UTC time
    current_time_utc = datetime.now(pytz.utc)

    # Convert the current time to PST (Pacific Standard Time)
    pst_timezone = pytz.timezone('US/Pacific')
    current_time_pst = current_time_utc.astimezone(pst_timezone)

    # Print the current timestamp in PST with proper formatting
    print("Current Timestamp in PST:", current_time_pst.strftime("%Y-%m-%d %H:%M:%S %Z"))

# Call the function to print the timestamp
print_current_timestamp_in_pst()

"""# << - - - - Prepare the data"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Set random seed for reproducibility
np.random.seed(10)

# Step 1: Load and clean the dataset
def load_and_clean_data(file_path, target_column, drop_columns_list):
    """
    Loads the dataset, drops irrelevant columns, calculates correlation matrix, and drops columns with high correlation.

    Args:
    - file_path (str): Path to the CSV file.
    - columns_to_drop (list): List of column names to drop initially.
    - target_column (str): Column to calculate correlation against.
    - drop_columns_list (list): Columns to drop based on analysis.

    Returns:
    - df (DataFrame): Cleaned dataframe.
    - X_train (DataFrame): Feature data without target column.
    - y_train (Series): Target column data.
    """
    # Load dataset
    df = pd.read_csv(file_path)

    # Drop irrelevant columns
    #AI - df = df.drop(columns=columns_to_drop, axis=1)

    # Display correlation matrix (optional step for analysis)
    corr_matrix = df.corr()
    print(corr_matrix[target_column].sort_values(ascending=False))

    # Drop columns based on analysis
    #AI - df = df.drop(columns=drop_columns_list, axis=1)

    # Split features and target
    X_train = df.drop(target_column, axis=1)
    y_train = df[target_column]

    return df, X_train, y_train

# Step 2: Convert atomic percentage to weight percentage
def convert_atomic_to_weight(X_train):
    """
    Molar mass is the mass of one mole of a given substance, which can be an
    element or a compound. It is typically expressed in grams per mole (g/mol).
    The molar mass of a substance is equivalent to the sum of the atomic masses of all atoms
    in a molecule of that substance, as represented in atomic mass units (amu), but
    converted to grams for use in macroscopic quantities.

    For elements:
    The molar mass of an element is numerically equal to its atomic mass
    (in atomic mass units), but in grams per mole. For example, the molar mass of carbon (C)
    is approximately 12.01 g/mol, because one mole of carbon atoms weighs about 12.01 grams.

    This function converts atomic percentages to weight percentages for relevant elements.
    currently, mol ratios written as a percentage of the whole. Dividing by 100 and multiplying
    by the molar mass gives the molar weight with respect to other elements

    Args:
    - X_train (DataFrame): Feature dataframe with atomic percentages.

    Returns:
    - X_train (DataFrame): Updated feature dataframe with weight percentages.
    """
    # Define atomic masses for each element
    atomic_masses = {
        'C(at%)': 12.01, 'Co(at%)': 58.93, 'Al(at%)': 26.98, 'V(at%)': 50.94,
        'Cr(at%)': 51.99, 'Mn(at%)': 54.94, 'Fe(at%)': 55.85, 'Ni(at%)': 58.69,
        'Cu(at%)': 63.55, 'Mo(at%)': 95.96
    }

    # Convert atomic percentages to weight percentages
    for element, mass in atomic_masses.items():
        X_train[element] = (X_train[element] / 100) * mass

    # Rename columns to indicate weight percentages
    X_train = X_train.rename(columns={f'{key}': key.replace('(at%)', '(wt)') for key in atomic_masses.keys()})

    X_train = X_train.rename(columns={
        'R(%)' : 'R',
        'CR(%)': 'CR'
    })
    # Normalize percentages for R and CR
    X_train['R'] = X_train['R'] / 100
    X_train['CR'] = X_train['CR'] / 100

    return X_train

# Step 3: Normalize features using MinMaxScaler
def normalize_features(X_train, y_train):
    """
    Normalizes the feature and target data using MinMaxScaler.

    Args:
    - X_train (DataFrame): Feature data.
    - y_train (Series): Target data (Yield Strength).

    Returns:
    - X_train_normalized (DataFrame): Normalized feature data.
    - y_train_normalized (DataFrame): Normalized target data.
    """
    scaler_minmax = MinMaxScaler()

    # Normalize features
    X_train_normalized = scaler_minmax.fit_transform(X_train)
    X_train_normalized = pd.DataFrame(X_train_normalized, columns=X_train.columns)

    # Normalize target
    y_train_normalized = scaler_minmax.fit_transform(y_train.values.reshape(-1, 1))
    y_train_normalized = pd.DataFrame(y_train_normalized, columns=['YS(Mpa)'])

    return X_train_normalized, y_train_normalized

# Step 4: Check for missing values (NaN) in the dataset
def check_missing_values(X_train):
    """
    Checks for missing values (NaN) in the dataset and prints out the columns with NaNs.

    Args:
    - X_train (DataFrame): Feature data.

    Returns:
    - None
    """
    nan_counts = X_train.isna().sum()
    columns_with_nan = X_train.columns[X_train.isna().any()].tolist()

    print("NaN counts per column:")
    print(nan_counts)
    print("\nColumns with NaN values:")
    print(columns_with_nan)

# Step 5: Split the data into training and testing sets
def split_data(X_train_normalized, y_train_normalized, test_size=0.2, random_state=42):
    """
    Splits the data into training and testing sets.

    Args:
    - X_train_normalized (DataFrame): Normalized feature data.
    - y_train_normalized (DataFrame): Normalized target data.
    - test_size (float): Proportion of data to include in the test set.
    - random_state (int): Random seed for reproducibility.

    Returns:
    - train_data (DataFrame): Training feature data.
    - test_data (DataFrame): Testing feature data.
    - train_labels (DataFrame): Training target data.
    - test_labels (DataFrame): Testing target data.
    """
    return train_test_split(X_train_normalized, y_train_normalized, test_size=test_size, random_state=random_state)

# Main function to execute the pipeline
def prep_hea_data():
    # File path to the dataset
    file_path = 'OpenCalphad_w_melting_point - A.csv'

    # Columns to drop during cleaning
    #columns_to_drop = ['Enthalpy_BCC', 'Enthalpy_HCP', 'G_RT_BCC', 'G_RT_HCP',
    #                   'dG_RT_(BCC - FCC)', 'dG_RT_(HCP - FCC)', 'dG_RT_(BCC - HCP)',
    #                   'dG_AT_(BCC - FCC)', 'dG_AT_(HCP - FCC)', 'dG_AT_(BCC - HCP)',
    #                   'H_RT_BCC', 'H_RT_HCP']

    # Target column for prediction
    target_column = 'YS(Mpa)'

    # Additional columns to drop based on correlation analysis
    drop_columns_list = ['phase_fraction_hcp']

    # Load and clean the data
    df, X_train, y_train = load_and_clean_data(file_path, target_column, drop_columns_list)

    # Convert atomic percentages to weight percentages
    X_train = convert_atomic_to_weight(X_train)

    # Normalize features and target
    X_train_normalized, y_train_normalized = normalize_features(X_train, y_train)

    # Check for missing values
    check_missing_values(X_train)

    # Split data into training and testing sets
    train_data, test_data, train_labels, test_labels = split_data(X_train_normalized, y_train_normalized)

    # Print shapes of train and test sets
    print("Training data shape:", train_data.shape)
    print("Test data shape:", test_data.shape)
    return train_data, test_data, train_labels, test_labels

train_data, test_data, train_labels, test_labels = prep_hea_data()
print_current_timestamp_in_pst()

"""# <<< Setup the SupervisedSparseAutoencoder class  >>>



"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
new_dataset = pd.read_csv('new_dataset -fixed.csv')
new_dataset = new_dataset[(new_dataset['Hom_Temp(K)']!=0) & (new_dataset['Anneal_Time(h)']!=0)]

print (new_dataset.head())

new_dataset.to_csv('new_dataset-check.csv')
#Step 2: Convert atomic percentage to weight percentage
def convert_atomic_to_weight(df):
    """
    Molar mass is the mass of one mole of a given substance, which can be an
    element or a compound. It is typically expressed in grams per mole (g/mol).
    The molar mass of a substance is equivalent to the sum of the atomic masses of all atoms
    in a molecule of that substance, as represented in atomic mass units (amu), but
    converted to grams for use in macroscopic quantities.

    For elements:
    The molar mass of an element is numerically equal to its atomic mass
    (in atomic mass units), but in grams per mole. For example, the molar mass of carbon (C)
    is approximately 12.01 g/mol, because one mole of carbon atoms weighs about 12.01 grams.

    This function converts atomic percentages to weight percentages for relevant elements.
    currently, mol ratios written as a percentage of the whole. Dividing by 100 and multiplying
    by the molar mass gives the molar weight with respect to other elements

    Args:
    - X_train (DataFrame): Feature dataframe with atomic percentages.

    Returns:
    - X_train (DataFrame): Updated feature dataframe with weight percentages.
    """
    # Define atomic masses for each element
    atomic_masses = {
        'C(at%)': 12.01, 'Co(at%)': 58.93, 'Al(at%)': 26.98, 'V(at%)': 50.94,
        'Cr(at%)': 51.99, 'Mn(at%)': 54.94, 'Fe(at%)': 55.85, 'Ni(at%)': 58.69,
        'Cu(at%)': 63.55, 'Mo(at%)': 95.96
    }

    # Convert atomic percentages to weight percentages
    for element, mass in atomic_masses.items():
        df[element] = (df[element] / 100) * mass

    # Rename columns to indicate weight percentages
    df = df.rename(columns={f'{key}': key.replace('(at%)', '(wt)') for key in atomic_masses.keys()})

    df = df.rename(columns={
        'R(%)' : 'R',
        'CR(%)': 'CR'
    })
    # Normalize percentages for R and CR
    df['R'] = df['R'] / 100
    df['CR'] = df['CR'] / 100



    return df

# Step 3: Normalize features using MinMaxScaler
def normalize_features(df):
    """
    Normalizes the feature and target data using MinMaxScaler.

    Args:
    - X_train (DataFrame): Feature data.
    - y_train (Series): Target data (Yield Strength).

    Returns:
    - X_train_normalized (DataFrame): Normalized feature data.
    - y_train_normalized (DataFrame): Normalized target data.
    """
    scaler = MinMaxScaler()

    # Normalize features
    df_normalized= scaler.fit_transform(df)
    df_normalized = pd.DataFrame(df_normalized, columns=df.columns)

    return df_normalized

new_dataset_adjusted = convert_atomic_to_weight(new_dataset)
new_dataset_normalized = normalize_features(new_dataset_adjusted)

print(new_dataset_normalized.head())

import tensorflow as tf
from tensorflow import keras

class SupervisedSparseAutoencoder:
    def __init__(self, hidden_dim, latent_dim, input_dim, target_sparsity=0.05):
        self.latent_dim = latent_dim
        self.hidden_dim = hidden_dim
        self.input_dim = input_dim
        self.target_sparsity = target_sparsity
        self._build_model()

    def _build_model(self):
        # Input layer
        input_data = keras.layers.Input(shape=(self.input_dim,))

        # Encoder
        encoded = keras.layers.Dense(self.hidden_dim, activation='selu')(input_data)
        latent = keras.layers.Dense(self.latent_dim, activation='sigmoid')(encoded)

        # Decoder
        decoded = keras.layers.Dense(self.hidden_dim, activation='selu')(latent)
        output_data = keras.layers.Dense(self.input_dim, activation='sigmoid')(decoded)

        # Prediction Layer (Supervised Component)
        prediction = keras.layers.Dense(1, activation='relu')(latent)

        # Autoencoder Model with Two Outputs
        self.autoencoder = keras.Model(inputs=input_data, outputs=[output_data, prediction])
        self.encoder = keras.Model(inputs=input_data, outputs=latent)

    def kl_divergence_loss(self, target_sparsity, actual_sparsity):
        return target_sparsity * tf.math.log(target_sparsity / (actual_sparsity + 1e-10)) + \
               (1 - target_sparsity) * tf.math.log((1 - target_sparsity) / (1 - actual_sparsity + 1e-10))

    def sparse_loss(self, y_true, y_pred):
        # Reconstruction loss (Mean Squared Error)
        reconstruction_loss = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)

        # Calculate the average activation of the latent layer
        latent_output = self.encoder(y_true)
        actual_sparsity = tf.reduce_mean(latent_output, axis=0)

        # KL Loss
        kl_loss = tf.reduce_sum(self.kl_divergence_loss(self.target_sparsity, actual_sparsity))
        return reconstruction_loss + kl_loss

    def compile(self, optimizer='adam', loss_weights=None):
        self.autoencoder.compile(optimizer=optimizer,
                                 loss=[self.sparse_loss, 'mean_squared_error'],
                                 loss_weights=loss_weights)  # Adjust loss weights as needed

    def fit(self, train_data, train_labels, epochs=10, batch_size=256, validation_data=None, callbacks=None):
        return self.autoencoder.fit(train_data,
                                    [train_data, train_labels],
                                    epochs=epochs,
                                    batch_size=batch_size,
                                    shuffle=False,
                                    validation_data=validation_data,
                                    callbacks=callbacks)

    def evaluate_sparsity(self, x_data):
        latent_representations = self.encoder.predict(x_data)
        sparsity = np.mean(np.abs(latent_representations) < 1e-3)
        return sparsity * 100

    def predict_latent(self, x_data):
        return self.encoder.predict(x_data)

    def decoded_output(self, x_data):
        return self.autoencoder.predict(x_data)

    def fine_tune_regression(self, train_data, train_labels, epochs=5, learning_rate=1e-4):
        """
        Option to fine-tune the latent space for the regression task.
        - Freeze encoder weights and only update the regression output layer.
        """
        for layer in self.encoder.layers:
            layer.trainable = False  # Freeze encoder layers

        # Compile again with a smaller learning rate and only train the regression output
        self.autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
                                 loss=[self.sparse_loss, 'mean_squared_error'],
                                 loss_weights=[0.4, 1.0])  # Equal weight for both losses

        # Fit only on regression
        self.autoencoder.fit(train_data,
                             [train_data, train_labels],  # Same inputs but focus is on regression
                             epochs=epochs,
                             shuffle=True)

"""# << - - -Now start the training - - - >

<<- - - Utility fn to commit training stats to file - - >>
"""

def append_training_stats_to_csv(best_result, filename='hea_model_training_results.csv'):

    from datetime import datetime
    import pytz
    import os
    import csv

    # Get the current time in PST
    pst = pytz.timezone('US/Pacific')
    current_time_pst = datetime.now(pst).strftime('%Y-%m-%d %H:%M:%S')

    # Prepare the row with the timestamp as the first field
    row = {'timestamp_PST': current_time_pst}

    # Add all key-value pairs from best_result to the row
    row.update(best_result)

    # Check if the file exists
    file_exists = os.path.isfile(filename)

    # If the file exists, read the current fieldnames (headers)
    if file_exists:
        with open(filename, mode='r') as file:
            reader = csv.DictReader(file)
            existing_fieldnames = reader.fieldnames
    else:
        existing_fieldnames = []

    # Combine the existing fieldnames with any new ones from the current row
    # Ensuring 'timestamp_PST' is always the first column
    fieldnames = ['timestamp_PST'] + [fn for fn in row.keys() if fn != 'timestamp_PST']

    # ** Inspect the fieldnames **
    #print("Fieldnames being used:")
    #print(fieldnames)

    # Open the file in append mode ('a') and create a CSV DictWriter
    with open(filename, mode='a', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        # If the file doesn't exist, write the header first
        if not file_exists:
            writer.writeheader()

        # Append the row of data
        writer.writerow(row)

"""<<- - - Actual training - - >>"""

from itertools import product
from datetime import datetime

print_current_timestamp_in_pst()

# Reshape labels for Keras (they need to be 2D arrays)
# Reshape labels for Keras (they need to be 2D arrays)
train_labels_autoencoder = train_labels.values.reshape(-1, 1) # Use .values to get the NumPy array from the DataFrame
test_labels_autoencoder = test_labels.values.reshape(-1, 1) # Use .values to get the NumPy array from the DataFrame

# Define the strategy
strategy = tf.distribute.MirroredStrategy()

# Define the parameter grid
param_grid = {
    'hidden_dim': [16, 18],
    'latent_dim': [6],
    'target_sparsity': [0.09],
    'batch_size': [4]
}

# Generate all combinations
param_combinations = list(product(param_grid['hidden_dim'],
                                  param_grid['latent_dim'],
                                  param_grid['target_sparsity'],
                                  param_grid['batch_size']))

# List to store results
results               = []
loss_weight_divergence= 0.6       #< - - - - - - - - - - - - - - - - - Set the Divergence loss weight here..
loss_weight_sparse    = 0.4       #< - - - - - - - - - - - - - - - - - Set the SPARSE loss weight here..


# Loop over all combinations
for hidden_dim, latent_dim, target_sparsity, batch_size in param_combinations:
    print(f'Training with hidden_dim={hidden_dim}, latent_dim={latent_dim}, '
          f'target_sparsity={target_sparsity}, batch_size={batch_size}')
    # Clear previous session
    tf.keras.backend.clear_session()

    with strategy.scope():
        # Build the model
        sparse_autoencoder = SupervisedSparseAutoencoder(input_dim=25,  # Adjust input_dim as needed
                                                         hidden_dim=hidden_dim,
                                                         latent_dim=latent_dim,
                                                         target_sparsity=target_sparsity)
        sparse_autoencoder.compile(optimizer='adam', loss_weights=[loss_weight_sparse, loss_weight_divergence])

        # Fit the model
        history = sparse_autoencoder.fit(train_data,
                                         train_labels_autoencoder,
                                         epochs=32,
                                         batch_size=batch_size,
                                         validation_data=(test_data, [test_data, test_labels_autoencoder]),
                                         callbacks=[tensorboard_callback])  # Include your callbacks if any

    # Get validation loss from history
    val_loss = history.history['val_loss'][-1]
    #val_reconstruction_loss = history.history['val_reconstruction_output_loss'][-1]
    #val_prediction_loss = history.history['val_prediction_output_loss'][-1]

    # Record the results
    results.append({
        'hidden_dim': hidden_dim,
        'latent_dim': latent_dim,
        'target_sparsity': target_sparsity,
        'batch_size': batch_size,
        'val_loss': val_loss
        #'val_reconstruction_loss': val_reconstruction_loss,
        #'val_prediction_loss': val_prediction_loss
    })

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Find the best combination
best_result = results_df.loc[results_df['val_loss'].idxmin()]

print("Best parameters:")
print(f"hidden_dim: {best_result['hidden_dim']}")
print(f"latent_dim: {best_result['latent_dim']}")
print(f"target_sparsity: {best_result['target_sparsity']}")
print(f"batch_size: {best_result['batch_size']}")
print(f"Validation loss: {best_result['val_loss']}")

best_result_copy = best_result.copy()

# Assuming best_result is a slice of a DataFrame and you want to add new values
best_result_copy['loss_weight_sparse'] = loss_weight_sparse
best_result_copy['loss_weight_divergence'] = loss_weight_divergence

print_current_timestamp_in_pst()

"""# - - - - - - - << Now instantiate an object of the model for the best hyper param's found from the grid searched model training, and fit the data"""



# Use the strategy scope
print_current_timestamp_in_pst()

with strategy.scope():
  sparse_autoencoder_to_disk = SupervisedSparseAutoencoder(input_dim=25, hidden_dim=int(best_result['hidden_dim']), latent_dim=int(best_result['latent_dim']), target_sparsity=best_result['target_sparsity'])
  sparse_autoencoder_to_disk.compile(optimizer='adam')
  history = sparse_autoencoder_to_disk.fit(train_data,
                                  train_labels_autoencoder,
                                  epochs=32,
                                  batch_size=4,
                                  validation_data=(test_data, [test_data, test_labels_autoencoder]),
                                  callbacks=[tensorboard_callback]
                                  )
  #sparse_autoencoder_to_disk.fine_tune_regression(train_data, train_labels, epochs=10, learning_rate=1e-4)

# Saving the entire autoencoder model and the encoder
#sparse_autoencoder_to_disk.autoencoder.save('./hea_sparse_autoencoder_full_model.h5')  # Save the full autoencoder model
#sparse_autoencoder_to_disk.encoder.save('./hea_sparse_autoencoder_encoder_model.h5')  # Save the encoder model separately

print_current_timestamp_in_pst()

"""# #- - - Load the model from the disk file.
# <<< Work In Progress >>>
"""

# Load the full autoencoder model
#sparse_autoencoder = keras.models.load_model('hea_sparse_autoencoder_full_model.h5', custom_objects={'sparse_loss': sparse_autoencoder.sparse_loss})
#sparse_autoencoder = sparse_autoencoder_to_disk
#Now get the latent values for this model
latent_train = sparse_autoencoder_to_disk.predict_latent(train_data)
latent_test = sparse_autoencoder_to_disk.predict_latent(test_data)

print_current_timestamp_in_pst()

latent_test_new =sparse_autoencoder_to_disk.predict_latent(new_dataset_normalized)

train_data.shape

"""# <<<< Run the regressors to compute the MSE's and R2's >>>>

# -- - - <<<< KNN regressor with Grid search & feature importances >>> - -
"""

# Import libraries
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# Extract latent representations
latent_train  = sparse_autoencoder_to_disk.encoder.predict(train_data)
latent_test   = sparse_autoencoder_to_disk.encoder.predict(test_data)

# Scale the latent features
#scaler = StandardScaler()
scaler = RobustScaler()
latent_train_scaled = scaler.fit_transform(latent_train)
latent_test_scaled = scaler.transform(latent_test)
latent_test_new_scaled = scaler.transform(latent_test_new)

# Define parameter grid
param_grid = {
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'n_neighbors': range(1, 51, 2),  # Test odd values from 1 to 49
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2, 3]  # For Minkowski metric with different powers
}


# Grid search on latent data
grid_search_knn = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn.fit(latent_train_scaled, train_labels)
best_knn = grid_search_knn.best_estimator_
print("Best KNN Hyperparameters:", grid_search_knn.best_params_)

# Evaluate best model on latent data

# Import libraries
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# Extract latent representations
latent_train  = sparse_autoencoder_to_disk.encoder.predict(train_data)
latent_test   = sparse_autoencoder_to_disk.encoder.predict(test_data)

# Scale the latent features
#scaler = StandardScaler()
scaler_robust = RobustScaler()
latent_train_scaled = scaler_robust.fit_transform(latent_train)
latent_test_scaled = scaler_robust.transform(latent_test)

# Define parameter grid
param_grid = {
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'n_neighbors': range(1, 51, 2),  # Test odd values from 1 to 49
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2, 3]  # For Minkowski metric with different powers
}


# Grid search on latent data
grid_search_knn = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn.fit(latent_train_scaled, train_labels)
best_knn = grid_search_knn.best_estimator_
print("Best KNN Hyperparameters:", grid_search_knn.best_params_)

# Evaluate best model on latent data

knn_predictions_test = best_knn.predict(latent_test_scaled)
knn_predictions_test_new  = best_knn.predict(latent_test_new_scaled)

print("\nBest KNN Regression Performance on Latent Test Data:")
print("MSE:", mean_squared_error(test_labels, knn_predictions_test))
print("RMSE:", np.sqrt(mean_squared_error(test_labels, knn_predictions_test)))
print("MAE:", np.mean(np.abs(test_labels - knn_predictions_test)))
print("R²:", r2_score(test_labels, knn_predictions_test))


'''
print("\nBest KNN Regression Performance on Random Conditional Test Data:")
print("MSE:", mean_squared_error(test_labels, knn_predictions_test_new))
print("RMSE:", np.sqrt(mean_squared_error(test_labels, knn_predictions_test_new)))
print("MAE:", np.mean(np.abs(test_labels - knn_predictions_test_new)))
print("R²:", r2_score(test_labels, knn_predictions_test_new))

'''

# - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
'''
# Step 2: Compute Permutation Importances
result = permutation_importance(
    estimator=best_knn,
    X=latent_test_scaled,
    y=test_labels,
    n_repeats=60,
    random_state=42,
    scoring='r2'
)
importances = result.importances_mean
std = result.importances_std

# Step 3: Interpret and Visualize
num_latent_features = latent_train_scaled.shape[1]
latent_feature_names = [f'Latent Feature {i+1}' for i in range(num_latent_features)]

# Create DataFrame
feature_importances = pd.DataFrame({
    'Feature': latent_feature_names,
    'Importance': importances,
    'Std': std
})
feature_importances.sort_values(by='Importance', ascending=False, inplace=True)

# Print feature importances
print("Permutation Feature Importances:")
print(feature_importances)

# Plot importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'], xerr=feature_importances['Std'])
plt.xlabel('Permutation Importance (Decrease in R²)')
plt.title('Feature Importances for KNN Regressor')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()


Now map Latent Features Back to Original Features
To understand how original features contribute to important latent features:

Inspect Encoder Weights: Analyze the weights of the encoder to see how original features map to latent features.
Correlation Analysis: Compute the correlation between original features and important latent features.

# Get encoder weights
encoder_weights = sparse_autoencoder_to_disk.encoder.get_weights
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# Extract latent representations
latent_train  = sparse_autoencoder_to_disk.encoder.predict(train_data)
latent_test   = sparse_autoencoder_to_disk.encoder.predict(test_data)

# Scale the latent features
#scaler = StandardScaler()
scaler = RobustScaler()
latent_train_scaled = scaler.fit_transform(latent_train)
latent_test_scaled = scaler.transform(latent_test)

# Define parameter grid
param_grid = {
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'n_neighbors': range(1, 51, 2),  # Test odd values from 1 to 49
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2, 3]  # For Minkowski metric with different powers
}


# Grid search on latent data
grid_search_knn = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn.fit(latent_train_scaled, train_labels)
best_knn = grid_search_knn.best_estimator_
print("Best KNN Hyperparameters:", grid_search_knn.best_params_)

# Evaluate best model on latent data

knn_predictions_test = best_knn.predict(latent_test_scaled)

print("\nBest KNN Regression Performance on Latent Test Data:")
print("MSE:", mean_squared_error(test_labels, knn_predictions_test))
print("RMSE:", np.sqrt(mean_squared_error(test_labels, knn_predictions_test)))
print("MAE:", np.mean(np.abs(test_labels - knn_predictions_test)))
print("R²:", r2_score(test_labels, knn_predictions_test))
'''


# - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

'''
Now map Latent Features Back to Original Features
To understand how original features contribute to important latent features:

Inspect Encoder Weights: Analyze the weights of the encoder to see how original features map to latent features.
Correlation Analysis: Compute the correlation between original features and important latent features.

# Get encoder weights
encoder_weights = sparse_autoencoder_to_disk.encoder.get_weights

# - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Scale original data
scaler_orig = scaler
train_data_scaled = scaler_orig.fit_transform(train_data)
test_data_scaled = scaler_orig.transform(test_data)

# Grid search on original data
grid_search_knn_orig = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn_orig.fit(train_data_scaled, train_labels)
best_knn_orig = grid_search_knn_orig.best_estimator_
print("\nBest KNN Hyperparameters on Original Data:", grid_search_knn_orig.best_params_)

# Evaluate best model on original data
knn_orig_predictions_test = best_knn_orig.predict(test_data_scaled)

print("\nBest KNN Regression Performance on Original Test Data:")
print("MSE:", mean_squared_error(test_labels, knn_orig_predictions_test))
print("RMSE:", np.sqrt(mean_squared_error(test_labels, knn_orig_predictions_test)))
print("MAE:", np.mean(np.abs(test_labels - knn_orig_predictions_test)))
print("R²:", r2_score(test_labels, knn_orig_predictions_test))

# Step 2: Compute Permutation Importances for Original Data
result_orig = permutation_importance(
    estimator=best_knn_orig,
    X=test_data_scaled,
    y=test_labels,
    n_repeats=60,
    random_state=42,
    scoring='r2'
)

importances_orig = result_orig.importances_mean
std_orig = result_orig.importances_std

# Step 3: Interpret and Visualize for Original Data
num_orig_features = train_data_scaled.shape[1]
orig_feature_names = train_data.columns  # Assuming your original features have names from the dataframe

# Create DataFrame for original feature importances
feature_importances_orig = pd.DataFrame({
    'Feature': orig_feature_names,
    'Importance': importances_orig,
    'Std': std_orig
})
feature_importances_orig.sort_values(by='Importance', ascending=False, inplace=True)

# Print feature importances for original data
print("Permutation Feature Importances for Original Data:")
print(feature_importances_orig)


# Plot importances for original data
plt.figure(figsize=(10, 6))
plt.barh(feature_importances_orig['Feature'], feature_importances_orig['Importance'], xerr=feature_importances_orig['Std'])
plt.xlabel('Permutation Importance (Decrease in R²)')
plt.title('Feature Importances for KNN Regressor (Original Data)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
'''

print_current_timestamp_in_pst()

print(knn_predictions_test_new)

df = pd.read_csv('new_dataset-check.csv')

# Filter predictions for high yield strength (e.g., above a certain threshold)

print(df.iloc[0])

# Assuming knn_predictions_test_new contains the predicted yield strengths
# Sort the predictions in descending order and get the indices of the top 2
top_2_indices = np.argsort(knn_predictions_test_new.ravel())[-2:]  # ravel to flatten the array

print(top_2_indices)


# Retrieve the corresponding rows from the dataset
top_2_heas = df.iloc[top_2_indices]
print(top_2_heas)


top_2_heas.to_csv('top_2_heas.csv', index=False)
# Print the top 2 HEAs

print(knn_predictions_test_new[977])
print(knn_predictions_test_new[1612])

from sklearn.preprocessing import MinMaxScaler, RobustScaler
import numpy as np

# Assuming knn_predictions_test_new contains the predicted yield strengths
# Sort the predictions in descending order and get the indices of the top 2
top_2_indices = np.argsort(knn_predictions_test_new.ravel())[-2:]  # ravel to flatten the array

# Retrieve the corresponding rows from the dataset
top_2_heas = new_dataset.iloc[top_2_indices]

# Save the top 2 HEAs to a CSV file
top_2_heas.to_csv('top_2_heas.csv', index=False)

# Print the top 2 HEAs
print(top_2_heas)

# Step 1: Prepare the predictions for inverse scaling
# Reshape the predictions to match the expected input shape for inverse_transform
reshaped_predictions = knn_predictions_test_new[top_2_indices].reshape(-1, 1)

# Step 2: Undo the RobustScaler (expects 6 features)
# Pad the predictions to 6 features (fill the rest with zeros or other placeholder values)
num_features_robust = 6  # Number of features for RobustScaler
padded_predictions_robust = np.zeros((reshaped_predictions.shape[0], num_features_robust))
padded_predictions_robust[:, 0] = reshaped_predictions.ravel()  # Fill the first column with the predictions

# Apply inverse RobustScaler
unscaled_latent_predictions = scaler_robust.inverse_transform(padded_predictions_robust)

# Step 3: Undo the MinMaxScaler (expects 25 features)
# Pad the unscaled latent predictions to 25 features (fill the rest with zeros or other placeholder values)
num_features_minmax = 25  # Number of features for MinMaxScaler
padded_predictions_minmax = np.zeros((unscaled_latent_predictions.shape[0], num_features_minmax))
padded_predictions_minmax[:, 0] = unscaled_latent_predictions[:, 0]  # Fill the first column with the predictions

# Apply inverse MinMaxScaler
unscaled_predictions = scaler_minmax.inverse_transform(padded_predictions_minmax)

# Extract the yield strength (first column) from the unscaled predictions
unscaled_yield_strengths = unscaled_predictions[:, 0]

# Print the unscaled predictions
for i, yield_strength in enumerate(unscaled_yield_strengths):
    print(f"Top {i+1} HEA - Unscaled Yield Strength: {yield_strength}")

# Import libraries
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# Extract latent representations
latent_train  = sparse_autoencoder_to_disk.encoder.predict(train_data)
latent_test   = sparse_autoencoder_to_disk.encoder.predict(test_data)

# Scale the latent features
#scaler = StandardScaler()
scaler = RobustScaler()
latent_train_scaled = scaler.fit_transform(latent_train)
latent_test_scaled = scaler.transform(latent_test)

# Define parameter grid
param_grid = {
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'n_neighbors': range(1, 51, 2),  # Test odd values from 1 to 49
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2, 3]  # For Minkowski metric with different powers
}


# Grid search on latent data
grid_search_knn = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn.fit(latent_train_scaled, train_labels)
best_knn = grid_search_knn.best_estimator_
print("Best KNN Hyperparameters:", grid_search_knn.best_params_)

# Evaluate best model on latent data

# Import libraries
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# Extract latent representations
latent_train  = sparse_autoencoder_to_disk.encoder.predict(train_data)
latent_test   = sparse_autoencoder_to_disk.encoder.predict(test_data)

# Scale the latent features
#scaler = StandardScaler()
scaler = RobustScaler()
latent_train_scaled = scaler.fit_transform(latent_train)
latent_test_scaled = scaler.transform(latent_test)

# Define parameter grid
param_grid = {
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'n_neighbors': range(1, 51, 2),  # Test odd values from 1 to 49
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2, 3]  # For Minkowski metric with different powers
}


# Grid search on latent data
grid_search_knn = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn.fit(latent_train_scaled, train_labels)
best_knn = grid_search_knn.best_estimator_
print("Best KNN Hyperparameters:", grid_search_knn.best_params_)

# Evaluate best model on latent data

knn_predictions_test = best_knn.predict(latent_test_scaled)

print("\nBest KNN Regression Performance on Latent Test Data:")
print("MSE:", mean_squared_error(test_labels, knn_predictions_test))
print("RMSE:", np.sqrt(mean_squared_error(test_labels, knn_predictions_test)))
print("MAE:", np.mean(np.abs(test_labels - knn_predictions_test)))
print("R²:", r2_score(test_labels, knn_predictions_test))


# - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Step 2: Compute Permutation Importances
result = permutation_importance(
    estimator=best_knn,
    X=latent_test_scaled,
    y=test_labels,
    n_repeats=60,
    random_state=42,
    scoring='r2'
)
importances = result.importances_mean
std = result.importances_std

# Step 3: Interpret and Visualize
num_latent_features = latent_train_scaled.shape[1]
latent_feature_names = [f'Latent Feature {i+1}' for i in range(num_latent_features)]

# Create DataFrame
feature_importances = pd.DataFrame({
    'Feature': latent_feature_names,
    'Importance': importances,
    'Std': std
})
feature_importances.sort_values(by='Importance', ascending=False, inplace=True)

# Print feature importances
print("Permutation Feature Importances:")
print(feature_importances)

# Plot importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'], xerr=feature_importances['Std'])
plt.xlabel('Permutation Importance (Decrease in R²)')
plt.title('Feature Importances for KNN Regressor')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

'''
Now map Latent Features Back to Original Features
To understand how original features contribute to important latent features:

Inspect Encoder Weights: Analyze the weights of the encoder to see how original features map to latent features.
Correlation Analysis: Compute the correlation between original features and important latent features.
'''
# Get encoder weights
encoder_weights = sparse_autoencoder_to_disk.encoder.get_weights
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# Extract latent representations
latent_train  = sparse_autoencoder_to_disk.encoder.predict(train_data)
latent_test   = sparse_autoencoder_to_disk.encoder.predict(test_data)

# Scale the latent features
#scaler = StandardScaler()
scaler = RobustScaler()
latent_train_scaled = scaler.fit_transform(latent_train)
latent_test_scaled = scaler.transform(latent_test)

# Define parameter grid
param_grid = {
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'n_neighbors': range(1, 51, 2),  # Test odd values from 1 to 49
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2, 3]  # For Minkowski metric with different powers
}


# Grid search on latent data
grid_search_knn = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn.fit(latent_train_scaled, train_labels)
best_knn = grid_search_knn.best_estimator_
print("Best KNN Hyperparameters:", grid_search_knn.best_params_)

# Evaluate best model on latent data

knn_predictions_test = best_knn.predict(latent_test_scaled)

print("\nBest KNN Regression Performance on Latent Test Data:")
print("MSE:", mean_squared_error(test_labels, knn_predictions_test))
print("RMSE:", np.sqrt(mean_squared_error(test_labels, knn_predictions_test)))
print("MAE:", np.mean(np.abs(test_labels - knn_predictions_test)))
print("R²:", r2_score(test_labels, knn_predictions_test))


# - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Step 2: Compute Permutation Importances
result = permutation_importance(
    estimator=best_knn,
    X=latent_test_scaled,
    y=test_labels,
    n_repeats=60,
    random_state=42,
    scoring='r2'
)
importances = result.importances_mean
std = result.importances_std

# Step 3: Interpret and Visualize
num_latent_features = latent_train_scaled.shape[1]
latent_feature_names = [f'Latent Feature {i+1}' for i in range(num_latent_features)]

# Create DataFrame
feature_importances = pd.DataFrame({
    'Feature': latent_feature_names,
    'Importance': importances,
    'Std': std
})
feature_importances.sort_values(by='Importance', ascending=False, inplace=True)

# Print feature importances
print("Permutation Feature Importances:")
print(feature_importances)

# Plot importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'], xerr=feature_importances['Std'])
plt.xlabel('Permutation Importance (Decrease in R²)')
plt.title('Feature Importances for KNN Regressor')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

'''
Now map Latent Features Back to Original Features
To understand how original features contribute to important latent features:

Inspect Encoder Weights: Analyze the weights of the encoder to see how original features map to latent features.
Correlation Analysis: Compute the correlation between original features and important latent features.
'''
# Get encoder weights
encoder_weights = sparse_autoencoder_to_disk.encoder.get_weights
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# Extract latent representations
latent_train  = sparse_autoencoder_to_disk.encoder.predict(train_data)
latent_test   = sparse_autoencoder_to_disk.encoder.predict(test_data)

# Scale the latent features
#scaler = StandardScaler()
scaler = RobustScaler()
latent_train_scaled = scaler.fit_transform(latent_train)
latent_test_scaled = scaler.transform(latent_test)

# Define parameter grid
param_grid = {
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'n_neighbors': range(1, 51, 2),  # Test odd values from 1 to 49
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2, 3]  # For Minkowski metric with different powers
}


# Grid search on latent data
grid_search_knn = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn.fit(latent_train_scaled, train_labels)
best_knn = grid_search_knn.best_estimator_
print("Best KNN Hyperparameters:", grid_search_knn.best_params_)

# Evaluate best model on latent data

knn_predictions_test = best_knn.predict(latent_test_scaled)

print("\nBest KNN Regression Performance on Latent Test Data:")
print("MSE:", mean_squared_error(test_labels, knn_predictions_test))
print("RMSE:", np.sqrt(mean_squared_error(test_labels, knn_predictions_test)))
print("MAE:", np.mean(np.abs(test_labels - knn_predictions_test)))
print("R²:", r2_score(test_labels, knn_predictions_test))


# - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Step 2: Compute Permutation Importances
result = permutation_importance(
    estimator=best_knn,
    X=latent_test_scaled,
    y=test_labels,
    n_repeats=60,
    random_state=42,
    scoring='r2'
)
importances = result.importances_mean
std = result.importances_std

# Step 3: Interpret and Visualize
num_latent_features = latent_train_scaled.shape[1]
latent_feature_names = [f'Latent Feature {i+1}' for i in range(num_latent_features)]

# Create DataFrame
feature_importances = pd.DataFrame({
    'Feature': latent_feature_names,
    'Importance': importances,
    'Std': std
})
feature_importances.sort_values(by='Importance', ascending=False, inplace=True)

# Print feature importances
print("Permutation Feature Importances:")
print(feature_importances)

# Plot importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'], xerr=feature_importances['Std'])
plt.xlabel('Permutation Importance (Decrease in R²)')
plt.title('Feature Importances for KNN Regressor')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

'''
Now map Latent Features Back to Original Features
To understand how original features contribute to important latent features:

Inspect Encoder Weights: Analyze the weights of the encoder to see how original features map to latent features.
Correlation Analysis: Compute the correlation between original features and important latent features.
'''
# Get encoder weights
encoder_weights = sparse_autoencoder_to_disk.encoder.get_weights

# - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Step 2: Compute Permutation Importances
result = permutation_importance(
    estimator=best_knn,
    X=latent_test_scaled,
    y=test_labels,
    n_repeats=60,
    random_state=42,
    scoring='r2'
)
importances = result.importances_mean
std = result.importances_std

# Step 3: Interpret and Visualize
num_latent_features = latent_train_scaled.shape[1]
latent_feature_names = [f'Latent Feature {i+1}' for i in range(num_latent_features)]

# Create DataFrame
feature_importances = pd.DataFrame({
    'Feature': latent_feature_names,
    'Importance': importances,
    'Std': std
})
feature_importances.sort_values(by='Importance', ascending=False, inplace=True)

# Print feature importances
print("Permutation Feature Importances:")
print(feature_importances)

# Plot importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'], xerr=feature_importances['Std'])
plt.xlabel('Permutation Importance (Decrease in R²)')
plt.title('Feature Importances for KNN Regressor')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

'''
Now map Latent Features Back to Original Features
To understand how original features contribute to important latent features:

Inspect Encoder Weights: Analyze the weights of the encoder to see how original features map to latent features.
Correlation Analysis: Compute the correlation between original features and important latent features.
'''
# Get encoder weights
encoder_weights = sparse_autoencoder_to_disk.encoder.get_weights()[0]  # Assuming the first weight matrix

# Extract weights corresponding to important latent features
top_k = 5  # Example: Analyze the top 5 features
important_latent_indices = feature_importances.index[:top_k]  # Indices of top k important latent features
important_weights = encoder_weights[:, important_latent_indices]

# Create a DataFrame
# original_feature_names = [...]  # List of original feature names - This was incomplete
original_feature_names = train_data.columns # Get the columns from the original dataframe
weights_df = pd.DataFrame(important_weights, index=original_feature_names, columns=[latent_feature_names[i] for i in important_latent_indices])

print("Weights from Original Features to Important Latent Features:")
print(weights_df)

#- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Scale original data
scaler_orig = scaler
train_data_scaled = scaler_orig.fit_transform(train_data)
test_data_scaled = scaler_orig.transform(test_data)

# Grid search on original data
grid_search_knn_orig = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_knn_orig.fit(train_data_scaled, train_labels)
best_knn_orig = grid_search_knn_orig.best_estimator_
print("\nBest KNN Hyperparameters on Original Data:", grid_search_knn_orig.best_params_)

# Evaluate best model on original data
knn_orig_predictions_test = best_knn_orig.predict(test_data_scaled)

print("\nBest KNN Regression Performance on Original Test Data:")
print("MSE:", mean_squared_error(test_labels, knn_orig_predictions_test))
print("RMSE:", np.sqrt(mean_squared_error(test_labels, knn_orig_predictions_test)))
print("MAE:", np.mean(np.abs(test_labels - knn_orig_predictions_test)))
print("R²:", r2_score(test_labels, knn_orig_predictions_test))

# Step 2: Compute Permutation Importances for Original Data
result_orig = permutation_importance(
    estimator=best_knn_orig,
    X=test_data_scaled,
    y=test_labels,
    n_repeats=60,
    random_state=42,
    scoring='r2'
)

importances_orig = result_orig.importances_mean
std_orig = result_orig.importances_std

# Step 3: Interpret and Visualize for Original Data
num_orig_features = train_data_scaled.shape[1]
orig_feature_names = train_data.columns  # Assuming your original features have names from the dataframe

# Create DataFrame for original feature importances
feature_importances_orig = pd.DataFrame({
    'Feature': orig_feature_names,
    'Importance': importances_orig,
    'Std': std_orig
})
feature_importances_orig.sort_values(by='Importance', ascending=False, inplace=True)

# Print feature importances for original data
print("Permutation Feature Importances for Original Data:")
print(feature_importances_orig)

'''
# Plot importances for original data
plt.figure(figsize=(10, 6))
plt.barh(feature_importances_orig['Feature'], feature_importances_orig['Importance'], xerr=feature_importances_orig['Std'])
plt.xlabel('Permutation Importance (Decrease in R²)')
plt.title('Feature Importances for KNN Regressor (Original Data)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
'''

print_current_timestamp_in_pst()

'''perform cross-validation to avoid overfitting'''
from sklearn.metrics import make_scorer, mean_squared_error, r2_score
import numpy as np

# Define a function to calculate RMSE and MAE as scorers
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# Define scoring metrics for cross-validation
scoring = {
    'MSE': make_scorer(mean_squared_error),
    'RMSE': make_scorer(rmse),
    'MAE': make_scorer(mae),
    'R2': make_scorer(r2_score)
}

# Perform cross-validation on the test set using the best model
from sklearn.model_selection import cross_validate

#first with original data
cv_results = cross_validate(
    best_knn_orig,
    train_data_scaled,
    train_labels,
    cv=10,
    scoring=scoring,
    n_jobs=-1
)

# Display cross-validation performance metrics
print("\nCross-Validation Performance on Original Test Data:")
for metric, scores in cv_results.items():
    if 'test' in metric:  # Filter only test scores
        metric_name = metric.split('_')[-1]
        print(f"{metric_name}: Mean={scores.mean():.4f}, Std={scores.std():.4f}")

#second on latent space data

cv_results = cross_validate(
    best_knn,
    latent_train_scaled,
    train_labels,
    cv=10,
    scoring=scoring,
    n_jobs=-1
)

# Display cross-validation performance metrics
print("\nCross-Validation Performance on Latent Test Data:")
for metric, scores in cv_results.items():
    if 'test' in metric:  # Filter only test scores
        metric_name = metric.split('_')[-1]
        print(f"{metric_name}: Mean={scores.mean():.4f}, Std={scores.std():.4f}")